{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import get_scheduler\n",
    "\n",
    "##### Own\n",
    "import train_utils.cifar_utils as cifar_utils\n",
    "from train_utils import make_optimizer, get_cfg\n",
    "\n",
    "from vision_transformer import VisionTransformer\n",
    "from train_utils import cifar_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg(\"config/vit_train.yml\")\n",
    "\n",
    "####### Dataset setup #######\n",
    "dataset_cfg = cfg[\"cifar_dataset\"]\n",
    "\n",
    "label2id, id2label = cifar_utils.get_label_dicts(dataset_cfg[\"label_type\"])\n",
    "\n",
    "train_dataloader, validation_dataloader, test_dataloader = cifar_utils.dataloaders_from_cfg(cfg)\n",
    "\n",
    "model = VisionTransformer(\n",
    "    image_size=cfg[\"cifar_dataset\"][\"image_size\"], use_linear_patch=True, num_classes=len(label2id.keys()))\n",
    "\n",
    "num_epochs = 1 # TODO: set a param in the config file\n",
    "lr = 0.003\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "optimizer = make_optimizer(optimizer_name='adamw',model=model, lr=0.003)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing single batch to model\n",
    "batch = next(iter(train_dataloader))\n",
    "pred = model(batch[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONXX\n",
    "input_val = batch\n",
    "torch.onnx.export(model, input_val , \"model.onnx\", input_names=['pixel_values'], output_names=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single epoch training loop\n",
    "model.train()\n",
    "epoch_train_loss = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    # transfer batch to device\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # forward pass and loss calculation\n",
    "    outputs = model(batch[\"pixel_values\"])\n",
    "    loss = loss_function(outputs, batch[\"coarse_label\"])\n",
    "    loss.backward()\n",
    "    epoch_train_loss += loss.item()\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "epoch_train_loss/=len(train_dataloader)\n",
    "\n",
    "# logger.info(f'Validating at epoch {epoch}')\n",
    "epoch_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_examples, correct_predictions= 0.0, 0.0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # transfer batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch[\"pixel_values\"])\n",
    "        \n",
    "        epoch_val_loss += loss_function(outputs, batch[\"coarse_label\"]).item() # retrieve only the scalar value\n",
    "        pred_labels = outputs.argmax(dim=1)\n",
    "        \n",
    "        total_examples += float(len(batch['coarse_label']))\n",
    "        correct_predictions += float((batch[\"coarse_label\"] == pred_labels).sum().item())\n",
    "\n",
    "    acc = correct_predictions / total_examples\n",
    "    epoch_val_loss /= len(train_dataloader)\n",
    "\n",
    "    print(f'-- train loss {train_loss:.3f} -- validation accuracy {acc:.3f} -- validation loss: {epoch_val_loss:.3f}')\n",
    "    if epoch_val_loss <= best_val_loss and save_model:\n",
    "        torch.save(model.state_dict(), 'model.pth')\n",
    "        best_val_loss = epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_val_loss = 0\n",
    "correct_predictions = 0\n",
    "# model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "batch = next(iter(train_dataloader))\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "print(f\"batch_size: {batch['pixel_values'].shape}\")\n",
    "print(f\"batch labels: {batch['coarse_label']}\")\n",
    "\n",
    "outputs = model(batch[\"pixel_values\"])\n",
    "\n",
    "loss = loss_function(outputs, batch[\"coarse_label\"])\n",
    "epoch_val_loss += loss.item() # retrieve only the scalar value\n",
    "pred_labels = outputs.argmax(dim=1)\n",
    "\n",
    "print(f\"outputs: {outputs.shape}\")\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"pred_labels: {pred_labels}\")\n",
    "\n",
    "correct_predictions += int((batch[\"coarse_label\"] == pred_labels).sum().item())\n",
    "print(f\"correct: {correct_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"coarse_label\"])\n",
    "    if batch_idx > 2:\n",
    "        break\n",
    "# batch = next(iter(train_dataloader))\n",
    "# # print((batch))\n",
    "# outputs = model(batch[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformer import VisionTransformer\n",
    "\n",
    "model = VisionTransformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
